# What is this?

It's a small node server that you can use as a proxy for LLMS.
The only tested LLM wrapper to use it with is Ollama so far.
It's pretty much only here to be used by Thule

# How do I use it?

1. downloadt the repo
2. "npm i" the dependencies
3. Make sure the LLM provider api is running and set the languageModelURL in proxy.js
4. "node proxy.js" to run it
